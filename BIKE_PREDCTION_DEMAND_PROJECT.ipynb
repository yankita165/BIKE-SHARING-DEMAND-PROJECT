{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yankita165/BIKE-SHARING-DEMAND-PROJECT/blob/main/BIKE_PREDCTION_DEMAND_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PROJECT NAME - BIKE SHARING DEMAND PREDICTION**"
      ],
      "metadata": {
        "id": "M4aKShty_PRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A bike sharing system is a new form of public transportation systems. Allowing its users to rent a bike from a location and return the bike to another location.\n",
        "* Historical data on bike rentals and environmental factors will be utilized to create a model that optimizes inventory management and enhances customer satisfaction in the bike rental industry.\n",
        "* By accurately predicting bike demand, this ML regression project aims to support bike rental companies in optimizing inventory management, reducing costs, and delivering better customer service. The insights gained from the project will enable data-driven decision, such as adjusting rental prices, resurce allocation, and improving overall operational efficiency.\n",
        "* The data set we will use for this project contains 14 columns as variables: Data, Seasons, Holiday, Functional day, Hour, Rainfall, Snowfall. Rented Bike Count, Temperature, Humidity, Dew Point Temperature, Visibility, Solar raidation and Windspeed.\n",
        "* We will use Python libraries such as Pandas, Seaborn, Numpy, and sklearn to develop our prediction algorithm. By testing and evaluating different models, we will determing which algorithm provide the most accurate predictions and can be deployed effectively in real world scenarios.\n",
        "* Today, there exists geat interest in these systems due to their importantt role in traffic, environmental and health issues. Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicity recorded in these systems. This feature turns bike sharing into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **GitHub Link**"
      ],
      "metadata": {
        "id": "fhV-wF8Ktjcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GitHub Link : https://github.com/yankita165/BIKE-SHARING-DEMAND-PROJECT"
      ],
      "metadata": {
        "id": "m7VD5OqiuONh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ilXWd9HtuNnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At present, numerous urban cities have implemented rental bikes as a means to improve mobility convience. Ensuring the availability and accessibility of rental bikes to the publiec in a timely manner is crucial, as it reduces waiting times. Consequently, the challenge lies in establishing a reliable and consistent supply of rental bikes for the city. The key aspect involves accurately predicting the number of bikes needed during each hour to maintain a stable provision of rental bikes.\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Description**  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.**\n",
        "\n",
        "\n",
        "**Attribute Information:**\n",
        "\n",
        "* Date : year-month-day\n",
        "\n",
        "* Rented Bike count - Count of bikes rented at each hour\n",
        "\n",
        "* Hour - Hour of the day\n",
        "\n",
        "* Temperature-Temperature in Celsius\n",
        "\n",
        "* Humidity - %\n",
        "\n",
        "* Windspeed - m/s\n",
        "\n",
        "* Visibility - 10m\n",
        "\n",
        "* Dew point temperature - Celsius\n",
        "\n",
        "* Solar radiation - MJ/m2\n",
        "\n",
        "* Rainfall - mm\n",
        "\n",
        "* Snowfall - cm\n",
        "\n",
        "* Seasons - Winter, Spring, Summer, Autumn\n",
        "\n",
        "* Holiday - Holiday/No holiday\n",
        "\n",
        "* Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Let's import the Modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Loading**"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Let's mount the google drive for imports the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Seoul bike data set from drive\n",
        "bike_df=pd.read_csv(\"/content/SeoulBikeData (4).csv\", encoding= 'latin')"
      ],
      "metadata": {
        "id": "uzQNLo_3SaXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset First View**"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the top 5 rows of the dataset\n",
        "bike_df.head()"
      ],
      "metadata": {
        "id": "z6p_mAAAT3yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the bottom 5 rows of the dataset\n",
        "bike_df.tail()"
      ],
      "metadata": {
        "id": "3V0_HFKgUKOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the shape of  dataset with rows and columns\n",
        "print(bike_df.shape)"
      ],
      "metadata": {
        "id": "Pnlm5jlTUlhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Columns count**"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting all the columns\n",
        "print(\"Features of the dataset\")\n",
        "bike_df.columns"
      ],
      "metadata": {
        "id": "vdZgYbYIVJ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking for the description of the dataset to get insights of the data\n",
        "bike_df.describe().T"
      ],
      "metadata": {
        "id": "A4o4mjnCWGPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the unique values\n",
        "bike_df.nunique()"
      ],
      "metadata": {
        "id": "hw_xDXb9W7lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Information**"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check details about the data set\n",
        "bike_df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Duplicate Values**"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the duplicate values\n",
        "dup=len(bike_df[bike_df.duplicated()])\n",
        "print(\"The number of duplicate values in the data set is = \",dup)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Check for count of missing values in each other\n",
        "bike_df.isna().sum()\n",
        "bike_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "missing = pd.DataFrame((bike_df.isnull().sum())*100/bike_df.shape[0]).reset_index()\n",
        "plt.figure(figsize = (16,5))\n",
        "ax = plt.stem(missing['index'], missing[0])\n",
        "plt.xticks(rotation = 90, fontsize = 7)\n",
        "plt.title(\"Percentage of Missing Values\")\n",
        "plt.ylabel(\"PERCENTAGE\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **In the above data we came to know that there are no missing and duplicate value present.**"
      ],
      "metadata": {
        "id": "nuOkq4G7VzFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename the complex columns name\n",
        "bike_df=bike_df.rename(columns={'Rented Bike Count':'Rented_Bike_Count',\n",
        "                                'Temperature(°C)':'Temperature',\n",
        "                                'Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'Wind_speed',\n",
        "                                'Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(°C)':'Dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
        "                                'Rainfall(mm)':'Rainfall',\n",
        "                                'Snowfall (cm)':'Snowfall',\n",
        "                                'Functioning Day':'Functioning_Day'})"
      ],
      "metadata": {
        "id": "TMuFKbwHWIhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This Dataset contains 8760 rows and 14 columns.**"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breaking date column"
      ],
      "metadata": {
        "id": "BIKptSEkbNKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the \"Date\" column into three \"year\", \"month\",column\n",
        "bike_df['Date'] = bike_df['Date'].apply(lambda x:\n",
        "                                        dt.datetime.strptime(x,\"%d/%m/%Y\"))\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['year'] = bike_df['Date'].dt.year\n",
        "bike_df['month'] = bike_df['Date'].dt.month\n",
        "bike_df['day'] = bike_df['Date'].dt.day_name()"
      ],
      "metadata": {
        "id": "G24BaKmvcXKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "bike_df['weekdays_weekend']=bike_df['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "bike_df=bike_df.drop(columns=['Date','day','year'],axis=1)"
      ],
      "metadata": {
        "id": "5OQQX2_cbLxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Essentially, when python reads the 'Date column, it interprets it as an object type, which is basically a string. Since the date column is crucial for analyzing user behaviour, it needs to be converted to a datetime format. Afterward, it can be split into three distinct columns, namely \"year\", \"month\", and \"day\", which can be categorized as a data type.**\n",
        "\n",
        "* **So we convert the \"date\" column into 3 different column i.e \"year\",\"month\", and \"day\".**"
      ],
      "metadata": {
        "id": "cCbiUOW4dbMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.head()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.info()"
      ],
      "metadata": {
        "id": "1jZxg404f5F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['weekdays_weekend'].value_counts()"
      ],
      "metadata": {
        "id": "tlzWF_tLrWNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.describe()"
      ],
      "metadata": {
        "id": "88Uy9YG3sPPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Variable Description**\n",
        "\n",
        "\n",
        "\n",
        "**Date** : The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formatting in DD/MM/YY, type:str, we need to convert into datetime format.\n",
        "\n",
        " **Rented Bike Coun**: Number of rented bikes per hour which our dependent variable and we need to predict that type: int.\n",
        "\n",
        "**Hour**: The hour of the day, starting from 0.23 it'S digital time format type: int, we need to convert it into category data type.\n",
        "\n",
        "**Temperature('c)**: Temperature in Celsius, type:Float\n",
        "\n",
        "**Humidity(%)**: Humidity in the air in %, type:int\n",
        "\n",
        "**Wind Speed**: Speed of the wind in m/s, type:Float.\n",
        "\n",
        "**Visibility(10m)**: Visibility in m, type:int\n",
        "\n",
        " **Dew pint temperature('c)**: Temperature at the beggining of the day, type:Float\n",
        "\n",
        " **Solar Radiation(MJ/M2)**: Sun contribution, type:Float\n",
        "\n",
        "**Rainfall(mm)**: Amount of raining in mm, type:Float\n",
        "\n",
        "**Snowfall(cm)**: Amount of snowing in cm, type:Float\n",
        "\n",
        "**Seasons**: Season of the year, type:str, there are only 4 seasons's in data\n",
        "\n",
        "**Holiday**:If the day is holiday period or not, type:str\n",
        "\n",
        "**Functioning Day**: If the day is a Functioning Day or not, type:str\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cheque Unique Values for each variable"
      ],
      "metadata": {
        "id": "sxkBpJqwAg9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "bike_df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Changing data type"
      ],
      "metadata": {
        "id": "9RwvdDZKA2JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the int64 column into category column\n",
        "cols=['Hour','month','weekdays_weekend']\n",
        "for col in cols:\n",
        "    bike_df[col]= bike_df[col].astype('category')"
      ],
      "metadata": {
        "id": "ZHWnBfFgBIuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check the result of data type\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "bt3cdUW9B3cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.columns"
      ],
      "metadata": {
        "id": "yJMy6m6UCFhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['weekdays_weekend'].unique()"
      ],
      "metadata": {
        "id": "43URSB-KCK5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis On The Data Set**\n"
      ],
      "metadata": {
        "id": "J8eOGpjpC68I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a dependent variable in data analysis?**\n",
        "\n",
        "  * **We analyse our dependent variable, A dependent variable is a variable whose value will change depending on the value of another variable.**\n",
        "  * **Our dependent variable is 'Rented Bike Count' so we need to analysis this column withh the other columns with the other columns by using some visualisation plot.**"
      ],
      "metadata": {
        "id": "QeMnmxK1DLro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Month"
      ],
      "metadata": {
        "id": "BOnsvfcCEzEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing dataset by visualisation\n",
        "fig, ax = plt.subplots(figsize=(15,8))\n",
        "sns.pointplot(data=bike_df,x='month', y='Rented_Bike_Count',ax =ax)\n",
        "ax.set(title='Count of Rented bikes according to Month')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BwgSrgUcDKqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Based on the point plot shown above, it is evident that the demand for rented bikes is higher from the months of May to October as compared to other months. It is worth noting that these months fall within the summer season.**"
      ],
      "metadata": {
        "id": "67JRpACgJYH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working  Day\n"
      ],
      "metadata": {
        "id": "Aj-pEi-ArOa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing dataset by visualisation\n",
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(10,8))\n",
        "sns.barplot(data=bike_df,x='Functioning_Day',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning Day')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wPAn7RbxD2cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing dataset by visualisation\n",
        "fig,ax=plt.subplots(figsize=(15,8))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='Functioning_Day',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning_Day')"
      ],
      "metadata": {
        "id": "QMFokoRTv9hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**. The bar plot and point displayed above  depict the utilization of rented bikes on working and non-working days.It is evident from the plot that people do not use rented bikes on non-functioning days.**"
      ],
      "metadata": {
        "id": "oqToxjRhxuhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hour"
      ],
      "metadata": {
        "id": "WPUQDSs9zbYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing dataset by visualisation\n",
        "fig,ax = plt.subplots(figsize=(15,8))\n",
        "sns.boxplot(data=bike_df,x='Hour', y='Rented_Bike_Count', ax=ax)\n",
        "ax.set(title='Count of Rented bikes according to Hour')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WKSyCuz2zisl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**. The plot above showcases the usage of rented bikes across different hours throughout the year. It is notable that people tend to use rented bikes during their working hours, specificially from 7AM and 9PM and 5AM to 7PM.**"
      ],
      "metadata": {
        "id": "pBsAAk9m1NPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Weekdays_weekend\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "axxTLtyu14u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysing dataset by visualisation\n",
        "fig,ax=plt.subplots(figsize=(10,8))\n",
        "sns.barplot(data=bike_df,x='weekdays_weekend',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes according to weekdays and weekend ')"
      ],
      "metadata": {
        "id": "evkr2QVD2trH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing dataset by visualisation\n",
        "fig,ax=plt.subplots(figsize=(15,8))\n",
        "sns.lineplot(data=bike_df,x='Hour',y= 'Rented_Bike_Count',hue='weekdays_weekend',ax=ax)\n",
        "ax.set(title='Count of Rented bikes according to weekdays_weekend')\n"
      ],
      "metadata": {
        "id": "4B1eNBl_5VJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Based on the line and bar plots above, we can observe that the demand for rented bikes is higher on weekdays, represented by the blue color, which is likely due to the increased demand for transportation to and from the office.\n",
        "The peak demand times during weekdays are between 7am-9am and 5pm-7pm. On weekends, represented by the orange color, the demand for rented bikes is generally lower, especially during the morning hours. However, in the evening, between 4pm-8pm, we can observe a slight increases in demand for rented bikes.**"
      ],
      "metadata": {
        "id": "_j_N8kMc7JnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Seasons"
      ],
      "metadata": {
        "id": "W4HfGI_p9aUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing data by visualisation\n",
        "fig,ax=plt.subplots(figsize=(15,8))\n",
        "sns.barplot(data=bike_df,x='Seasons', y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes according to Seasons')"
      ],
      "metadata": {
        "id": "7eO_e23p9ebz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing dataset by visualisation\n",
        "fig,ax=plt.subplots(figsize=(15,8))\n",
        "sns.pointplot(data=bike_df,x='Hour', y='Rented_Bike_Count',hue = 'Seasons',ax=ax)\n",
        "ax.set(title='Count of Rented bikes according to seasons')\n"
      ],
      "metadata": {
        "id": "VcaEnL1l-YQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The bar plot and point plot presented above depict the usage of rented bikes across four distinct seasons. The analysis reveals that the use of rented bikes in significantly high during the summer season with peak demand during 7am-9am and 5pm-7pm. However, during the winter season, teh use of rented bikes is quite low due to snowfall.**"
      ],
      "metadata": {
        "id": "La4GBgvg_2D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Holiday"
      ],
      "metadata": {
        "id": "A7LbGISPArJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing dataset by visualisation\n",
        "fig,ax=plt.subplots(figsize=(10,5))\n",
        "sns.boxplot(data=bike_df,x='Holiday',y='Rented_Bike_Count',ax=ax)\n",
        "ax.set(title='Count of Rented bikes according to Holiday')"
      ],
      "metadata": {
        "id": "FotACnibAuNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysing dataset by visualisation\n",
        "fig,ax=plt.subplots(figsize=(15,8))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue ='Holiday',ax=ax)\n",
        "ax.set(title='Count of Rented bikes according to Holiday')"
      ],
      "metadata": {
        "id": "MCBxyypxBaNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The bar plot and point plot displayed above illustrate the usage of rented bikes during holidays, indicating that people tend to use rented bikes primarily between 2pm to 8pm.**"
      ],
      "metadata": {
        "id": "OWC7ZHs5CZTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analyze of Numerical variables**"
      ],
      "metadata": {
        "id": "9Pk_FJwk2zb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign the numerical column to variable\n",
        "num_columns=list(bike_df.select_dtypes(['Int64','float64']).columns)\n",
        "num_features=pd.Index(num_columns)\n",
        "num_features"
      ],
      "metadata": {
        "id": "eHZSvokb26zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing disputes to analyse the distribution of all numerurical features\n",
        "for col in num_features:\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.distplot(x=bike_df[col])\n",
        "  plt.xlabel(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fe-Eh9wK4Jpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Numerical vs Rented_Bike_Count"
      ],
      "metadata": {
        "id": "Mryr9EdU48qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the plot to analyse teh relationship between 'Rented_Bike_Count' and 'Temperature'\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='Temperature',y='Rented_Bike_Count',data=bike_df)\n",
        "plt.title('Temperature vs Rented Bike Count')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "mVtBV9PG5IdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The plot above indicates that individuals tend to prefer biking when the temperature is relatively high, averaging around 25'c.**"
      ],
      "metadata": {
        "id": "8VYKH5aq6n7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the plot to analyze  the relationship between 'Rented_Bike_Count' and 'Solar_Radiation'\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='Solar_Radiation', y='Rented_Bike_Count', data=bike_df)\n",
        "plt.title('Solar_Radiation bs Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1HeMgsiY7E0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The plot above indicates that the number of rented bikes significantly increases with the presence of solar radiation, reaching a count of approximately 1000.**"
      ],
      "metadata": {
        "id": "NTBzKR-H74vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the plot to analyze the relationship between 'Rented_Bike_Count' and 'Rainfall\n",
        "bike_df.groupby('Rainfall').mean()['Rented_Bike_Count'].plot()\n"
      ],
      "metadata": {
        "id": "w06vkn0nFcF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The above plot indicates that despite heavy rainfall, the demand for related bikes does not decrease. For instance, even with a rainfall of 20mm, there is a significant peak in the number of rented bikes.**"
      ],
      "metadata": {
        "id": "_DajqGi_GABS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the plot to analyze the relationship between 'Rented_Bike_Count' and 'Snowfall'\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='Snowfall', y='Rented_Bike_Count',data=bike_df)\n",
        "plt.title('Snowfall vs Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XDlpzX_0GtsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The plot indicates that when the snowfall is more than 4cm, there is a significant drop in the number of rented bikes, as shown on the y-axis.**"
      ],
      "metadata": {
        "id": "i1O5WyYvH0wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Wind_speed\"\n",
        "bike_df.groupby('Wind_speed').mean()['Rented_Bike_Count'].plot()\n",
        "\n"
      ],
      "metadata": {
        "id": "Jyh8rcRsIKtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **From the plot above, we can observe that the demand for rented bikes is evenly distributed regardless of the wind speed. However, there is a spike in bike rentals when the wind speed is at 7m/s, indicating that people enjoy riding bikes when there is a slight breeeze.**"
      ],
      "metadata": {
        "id": "owq3GfkgJ-3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression plot**"
      ],
      "metadata": {
        "id": "9G25bPHLf8HU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Seaborn regression plots are designed to aid in exploratory data analysis by providing a visual aid that highlights patterns in a dataset. These plots, as their name implies, generate a regression line between two variables and assist in visualizing their linear relationship.**"
      ],
      "metadata": {
        "id": "cjLkdUTEgEcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the regression plot for all the numerical features\n",
        "for col in num_features:\n",
        "  fig,ax=plt.subplots(figsize=(10,6))\n",
        "  sns.regplot(x=bike_df[col],y=bike_df['Rented_Bike_Count'],scatter_kws={\"color\": 'red'}, line_kws={\"color\": \"black\"})\n"
      ],
      "metadata": {
        "id": "5Rcz_ZMRgnZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * **The above regression plot for all the numerical features indicates that 'Temperture', 'Wind_speed', 'Visibility', 'Dew_point_temperature', and 'Solar_Radiation' are positively correlated with the target variable, that is, an increase in these features results in an increase in rented bike count. On the other hand, 'Rainfall', 'Snowfall', and 'Humidity' are negatively correlated with the target variable, indicating that an increase in these features results in a decrease in rented bike count.**"
      ],
      "metadata": {
        "id": "G1WDOFT4iuY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  ***Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Normalise Rented_Bike_Count column data**\n"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution plot of Rented Bike Count\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.xlabel('Rented_Bike_Count')\n",
        "plt.ylabel('Density')\n",
        "ax=sns.distplot(bike_df['Rented_Bike_Count'],hist=True ,color=\"y\")\n",
        "ax.axvline(bike_df['Rented_Bike_Count'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(bike_df['Rented_Bike_Count'].median(), color='black', linestyle='dashed', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Based on the graph above, it can be observed that the Rented Bike Count has a moderately skewed distribution towards the right. However,since the assumption for linear regression is that the dependent variables's distribution should be normal, we need to apply some transformation to achieve normality.**"
      ],
      "metadata": {
        "id": "KIen8llCm1fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot of Rented Bike Count to check outliers\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=bike_df['Rented_Bike_Count'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * **The above boxplot shows that we have detect outliers in Rented Bike Count column**"
      ],
      "metadata": {
        "id": "n2Ch97Qxoej6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying square root to Rented Bike Count to improve skewness\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "ax=sns.distplot(np.sqrt(bike_df['Rented_Bike_Count']), color=\"y\")\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).mean(), color='green', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Applying the generic rule of taking the square root of skewed variable to normalize them, we can observe that the Rented Bike Count, which was previously skewed, now follows a nearly normal distribution.**"
      ],
      "metadata": {
        "id": "-reEkXelppv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #After applying sqrt on Rented Bike Count check wheater we still have outliers\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=np.sqrt(bike_df['Rented_Bike_Count']))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.corr()"
      ],
      "metadata": {
        "id": "QBd25iWaq9p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **After applying Square root to the Rented Bike Count column, we find that there is no outliers present.**"
      ],
      "metadata": {
        "id": "GTDcCHxErFZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Checking of correlation between variables**"
      ],
      "metadata": {
        "id": "rwV0UQ8iCO9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking in OLS Model\n",
        "\n",
        "**Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable**"
      ],
      "metadata": {
        "id": "QMfOcHgHrtIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing  the module\n",
        "#assign the 'x','y' value\n",
        "#Checking in OLS Model\n",
        "import statsmodels.api as sm\n",
        "X = bike_df[[ 'Temperature','Humidity',\n",
        "       'Wind_speed', 'Visibility','Dew_point_temperature',\n",
        "       'Solar_Radiation', 'Rainfall', 'Snowfall']]\n",
        "Y = bike_df['Rented_Bike_Count']\n",
        "bike_df.head()"
      ],
      "metadata": {
        "id": "3pBD8rqfsIxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add a constant column\n",
        "X = sm.add_constant(X)\n",
        "X"
      ],
      "metadata": {
        "id": "4zAndobPsr8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting a OLS model\n",
        "model= sm.OLS(Y, X).fit()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **The R sqauare and Adjacent Square are close to each other, indicating that the model is explained about  40% of the variance in the Rented Bike count. The P value for F statistic is less than 0.05 for a 5% level of significance. However,the P valuess for dew point temp and visibility are quite high, indicating that these variables are not significant.**\n",
        "\n",
        "* **The Omnibus tests checks the skewness and kurtosis of the residuals, and in this case, the value of Omnibus is high, indicating that there is skewness in the data. The condition number is large, 3.11e+04, suggest  that there may be strong multicollinearity or other numerical issues.**\n",
        "\n",
        "* **The Durbin-Watson test is used to detect autocorrelation among variables,and in this case, the  value is less than 0.5, indicating the presence of  positive auto correlation among the variables.**"
      ],
      "metadata": {
        "id": "PgMexnVPuA3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.corr()"
      ],
      "metadata": {
        "id": "GveSgCUM2JBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Based on the OLS Model, it was found that there is a high correlation between 'Temperture' and 'Dew_point_temperature'. Therefore, one of these variables needs to be dropped. To decide which one to drop, (P>|t|) values from the  above table were checked. It was found that the 'Dew_point_temperature' value is higher, indicating that it is less significant. Therefore Dew_point_temperature column was dropped. To make this decision clearer, a  heatmap visualization was used in next step**"
      ],
      "metadata": {
        "id": "avJq9itJ5IrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heatmap\n",
        "\n",
        "* **A correlation heatmap is a type of graphical representation that displays the correlation matrix, which helps to determine the correlation between different variables.**"
      ],
      "metadata": {
        "id": "KqWe8of57Tzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Correlation matrix\n",
        "plt.figure(figsize=(20,8))\n",
        "correlation=bike_df.corr()\n",
        "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
        "sns.heatmap((correlation),mask=mask, annot=True,cmap='coolwarm')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can observe on the heatmap that on the target variable line the most positively correlated variables to the rent are:**\n",
        "* the temperture\n",
        "* the dew point temperature\n",
        "* the solar radiation\n",
        "\n",
        "**And most negatively correlated variables are:**\n",
        "* Humidity\n",
        "* Rianfall\n",
        "\n",
        "* **Based on the correlation heatmap above, we observe that columns 'Temperture' and 'Dew point temperture' are positively correlated, with a correlation coefficient of 0.91. Therefore, dropping the 'Dew point temperature('C') column would not significantly affect our analysis since it has similar variations to 'Temperture'.**"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the Dew point temperature column\n",
        "bike_df=bike_df.drop(['Dew_point_temperature'],axis=1)"
      ],
      "metadata": {
        "id": "AKktOHoy_q9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.info()"
      ],
      "metadata": {
        "id": "iZez9gIl_uuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering & Data Pre-Processing**"
      ],
      "metadata": {
        "id": "7SyxQegJDDwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **A dataset may contain various type of values, sometimes it consists of categorical values. So, in-order to use those categorical value for programming efficiently we create dummy variables.**"
      ],
      "metadata": {
        "id": "rcNDRi5CDXn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign all categorical features to a variable\n",
        "cat_features=list(bike_df.select_dtypes(['object','category']).columns)\n",
        "cat_features=pd.Index(cat_features)\n",
        "cat_features\n"
      ],
      "metadata": {
        "id": "NpCbMsKBDyKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Onehot encoding enables a more description representation of categorical data. Since many machines learning algorithm do not accept categorical data as input or output variables, the categorical need to be converted into numerical values.**"
      ],
      "metadata": {
        "id": "-0p1fOqBEhsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a copy\n",
        "bike_df_copy = bike_df\n",
        "\n",
        "def one_hot_encoding(data, column):\n",
        "    data = pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)\n",
        "    data = data.drop([column], axis=1)\n",
        "    return data\n",
        "\n",
        "for col in cat_features:\n",
        "    bike_df_copy = one_hot_encoding(bike_df_copy, col)\n",
        "bike_df_copy.head()"
      ],
      "metadata": {
        "id": "wWtT_5UZFFxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train Test split for regression**\n",
        "\n",
        "**It is generally recommended to divide the dataset into two parts, namely training and testing sets, before applying any model. This division involves allocting some proportion of the data for training the model and reserving the remaining portion for evaluating the model's performance on unseen data. The proportion of data allocated for training and testing can vary from person to person, with commonly used ratios being 60:40,70:30,750:25 or 80:20 for training and testing respectively. To perform this split, we will use the scikit learn library.**"
      ],
      "metadata": {
        "id": "xL3Un3TYF5wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign the value  in X and Y\n",
        "X = bike_df_copy.drop(columns=['Rented_Bike_Count'], axis=1)\n",
        "y = np.sqrt(bike_df_copy['Rented_Bike_Count'])"
      ],
      "metadata": {
        "id": "Q8qrMKeiIsr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "MG8DhVv-JMom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "egVgWib1Jk3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating test and train data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "metadata": {
        "id": "iOxRO-wxJvtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df_copy.describe().columns"
      ],
      "metadata": {
        "id": "4crreJBmK2ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The mean squared error (MSE) tells you how to close a regression line is to a set to points. It does this by taking the distances from the points to the regression line (these distances are the \"errors\") and squaring them. It's called the mean squared error as you're finding the average of a set of errors. The lower the MSE, the better the forecast.\n",
        "\n",
        "* MSE formula = (1/n) * Σ(actual – forecast)2\n",
        "Where:\n",
        "\n",
        "n = number of items,\n",
        "\n",
        "Σ = summation notation,\n",
        "\n",
        "Actual = original or observed y-value,\n",
        "\n",
        "Forecast = y-value from regression.\n",
        "\n",
        "Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors).\n",
        "\n",
        "Mean Absolute Error (MAE) are metrics used to evaluate a Regression Model. ... Here, errors are the differences between the predicted values (values predicted by our regression model) and the actual values of a variable.\n",
        "\n",
        "R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
        "\n",
        "Formula for R-Squared\n",
        "                         \n",
        "                         R² = 1 - unexplained Variation / Total Variation\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "R\n",
        "2 =1− Total Variation Unexplained Variation​\n",
        "\n",
        "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
        "​"
      ],
      "metadata": {
        "id": "aLIlhDnhLESW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ML Model Implementation**"
      ],
      "metadata": {
        "id": "dNlvohk3EFSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LINEAR REGRESSION**"
      ],
      "metadata": {
        "id": "e5kUrCGFMJhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression models describe the relationship between variables by fitting a line to the observed data. Linear regression models use a straight line.\n",
        "Linear regression uses a linear approach to model the relationship between independent and dependent variables. In simple words its a best fit line drawn over the values of independeent variables and dependent variable. In case of single variable, the formula is same as straight line equation having  an intercept and slope.\n",
        "                              y_pred = β + βx\n",
        "\n",
        "                         β and  β\n",
        " are intercept and slope respectively.\n",
        " In case of multiple features the formula translates into\n",
        "                                    y_pred =                         \n",
        "  where x_1x_2x_3 are the features values and\n",
        "\n",
        "  are weights assignes to each of the features. These become the parameters which the algorithm tries to learn using Gradient descent.\n",
        "  Gradient descent is the process by which the algorithm tries to update the parameters using a loss function. Loss function is nothing but the different between the actual values and predicted values(aka error or residuals). There are different types of loss function but this is the simplest one. Loss function summed over all observation gives the cost functions. The role of gradient descent is to update the parameters till the cost function is minimized i.e, a global minima is reached. It uses a hyperparameter 'alpha' that gives a weightage to the cost function and decides on how big the steps to take. Alpha is called as the learning rate. It is always necessary to keep an optimal value of alpha as high and low values of alpha might the gradient descent overshoot or get stuck at a local minima. There are also some basic assumptions that must be fulfilled before implementing this algorithm. They are:\n",
        "  \n",
        "  1. No multicollinearity in the dataset.\n",
        "\n",
        "  2.Independent variables should show linear relationship with dv.\n",
        "\n",
        "  3. Residual mean should be 0 or close to 0.\n",
        "\n",
        "  4.There should be no heteroscedasticity i.e., variance should be constant along the line of best fit.\n",
        "\n",
        "Let us now implement our first model. We will be using LinearRegression from scikit library.\n",
        "\n",
        "                         "
      ],
      "metadata": {
        "id": "7aptljGWMQbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg= LinearRegression().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "reg.score(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "FzQmvAohf5FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the coefficient\n",
        "reg.coef_"
      ],
      "metadata": {
        "id": "wYgTA_BhjwEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the x_train and x_test value\n",
        "y_pred_train=reg.predict(X_train)\n",
        "y_pred_test=reg.predict(X_test)"
      ],
      "metadata": {
        "id": "Mh59nBRYosbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error((y_train), (y_pred_train))\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_train, y_pred_train)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score(y_train, y_pred_train)\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "MSE : 35.07751288189293\n"
      ],
      "metadata": {
        "id": "MnFYvA3YpDZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **It appears that the  r2 score of our model is 0.77, indicating that the model has  captured a significant portion of the most data variance. We should store this result  in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "cKzLk-7rpdBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "training_df=pd.DataFrame(dict1,index=[1])\n"
      ],
      "metadata": {
        "id": "zySGkCqe6YNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error(y_test, y_pred_test)\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_test, y_pred_test)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score((y_test), (y_pred_test))\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score((y_test), (y_pred_test)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",Adjusted_R2_lr )"
      ],
      "metadata": {
        "id": "2MXjGNL4-2ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.78. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "XUlkrHDE_ahh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "test_df=pd.DataFrame(dict2,index=[1])"
      ],
      "metadata": {
        "id": "vlYqAOE1_niN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test),(y_test)-(y_pred_test),color='black')\n",
        "plt.xlabel('Preddicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lgZzkpUL_ux0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "plt.scatter(range(len(y_pred_test)),y_pred_test, s=20, c='blue', label = 'Predicted')\n",
        "plt.scatter(range(len(y_test)), y_test, s=20, c ='red', label = 'Actual')\n",
        "plt.legend()\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tg-xf_8VApS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LASSO REGRESSION**"
      ],
      "metadata": {
        "id": "2YyPkohfB8rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an instances of Lasso Regression Implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha=1.0, max_iter=3000)\n",
        "# Fit the Lasso model\n",
        "lasso.fit(X_train, y_train)\n",
        "# Create the model score\n",
        "print(lasso.score(X_test, y_test), lasso.score(X_train, y_train))"
      ],
      "metadata": {
        "id": "K2jJnWlcB7vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_lasso=lasso.predict(X_train)\n",
        "y_pred_test_lasso=lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "7M6MqcYQDhy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_l= mean_squared_error((y_train), (y_pred_train_lasso))\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_train, y_pred_train_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score(y_train, y_pred_train_lasso)\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "qAi5HbQoDlzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our model's r2 score value is 0.40, indicating it is unable to capture a significant portion of thr data variance. We should save the score in a dataframe for future comparisons.**"
      ],
      "metadata": {
        "id": "PyhEfbFCDqVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2)\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "XucAwH-sDwUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_l= mean_squared_error(y_test, y_pred_test_lasso)\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_test, y_pred_test_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score((y_test), (y_pred_test_lasso))\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l=(1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5DT15J2QE8O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.38. This means our linear model is not performing well on the data. To investigate further, we will examine the residuals and check for heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "q6cmxIsQFgHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "bZSKUK8LGPxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(np.array(y_pred_test_lasso))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PZv9YH5bGXBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_lasso),(y_test)-(y_pred_test_lasso),color = 'Indigo')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3sU4ZtWEGml_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RIDGE REGRESSION**"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the packages\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge = Ridge(alpha=0.1)"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "ridge.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "Au7kDZ1jmdaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the score\n",
        "ridge.score(X_train,y_train)"
      ],
      "metadata": {
        "id": "JuURbsXMnFIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the X_train and X_test value\n",
        "y_pred_train_ridge=ridge.predict(X_train)\n",
        "y_pred_test_ridge=ridge.predict(X_test)\n"
      ],
      "metadata": {
        "id": "jk1AkwtsnOmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_r= mean_squared_error((y_train), (y_pred_train_ridge))\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_train, y_pred_train_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score(y_train, y_pred_train_ridge)\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "dE0WTWPjn1h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It appears that our model has an r2 score value of 0.77, incicating that it can capture a significant portion of the data variance. We can store this result in a duration for future comparisons.**"
      ],
      "metadata": {
        "id": "viNLyNDJocIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)\n"
      ],
      "metadata": {
        "id": "Owq1iIVJo4_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_r= mean_squared_error(y_test, y_pred_test_ridge)\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_test, y_pred_test_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score((y_test), (y_pred_test_ridge))\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "NRkicj-QpL9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The test set has an r2 score of 0.78, indicating good performance by our linear model on the data. However, we need to examine the residuals visually to check for heteroscedasticity, which refers to unequal variance or scatter.**"
      ],
      "metadata": {
        "id": "l_GzsNBGpu3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "ie6DXAsdqdc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.scatter(range(len(y_pred_test_ridge)),y_pred_test_ridge, s=20, c='blue', label='Predicted')\n",
        "plt.scatter(range(len(y_test)),y_test, s=20, c='red', label='Actual')\n",
        "plt.legend\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2qI9_g3cqsdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Heteroscadacity\n",
        "plt.scatter((y_pred_test_ridge),(y_test)-(y_pred_test_ridge),color='blue')\n",
        "plt.xlabel('Predicted value')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IAASFCpisSqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ELASTIC NET REGRESSION**"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the packages\n",
        "from sklearn.linear_model import ElasticNet\n",
        "#a * L1 + b * L2\n",
        "#alpha = a + b and l1_ratio = a / (a + b)\n",
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "elasticnet.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "L_toA3Pfti0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the score\n",
        "elasticnet.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "Y2TFZbEBtuPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the x_train and x_test value\n",
        "y_pred_train_en=elasticnet.predict(X_train)\n",
        "y_pred_Test_en=elasticnet.predict(X_test)"
      ],
      "metadata": {
        "id": "XtuUJSy_t6a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#Calculate MSE\n",
        "MSE_e= mean_squared_error((y_train), (y_pred_train_en))\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_train, y_pred_train_en)\n",
        "print(\"MAE :\",MAE_e)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score(y_train, y_pred_train_en)\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "-kNMIxHGugx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on the r2 score value of 0.62, it appears that our model has successfully captured a significant portion of the data variance. We can store this data in a dataframe for future comparisons.**"
      ],
      "metadata": {
        "id": "diZ_y_DZvDUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing the test set metrics value is a dataframe for later comprison\n",
        "dict1 = {'Model':'Elastic net regression ',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "ghPvcztLvf-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "#Calculate MSE\n",
        "MSE_e= mean_squared_error((y_train), (y_pred_train_en))\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_train, y_pred_train_en)\n",
        "print(\"MAE :\",MAE_e)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score(y_train, y_pred_train_en)\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "Z6NClKMUAQeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**The test set's r2_score of 0.62 indicates that  our linear model is effectively modeling  the data. However, we need to investigate if there is heteroscedasticity which refers to enequal variance or scatter, by examining the residuals visually.**"
      ],
      "metadata": {
        "id": "sL0weCeNEFCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing the test set metrics value is a dataframe for later comparison\n",
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Elastic net regression Test',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "JbyA4OYaICFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(np.array(y_pred_Test_en))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend(['Predicted','Actual'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WJy92oNfIdI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Heteroscadacity\n",
        "plt.scatter((y_pred_Test_en),(y_test)-(y_pred_Test_en), color='green')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IxQ3V8IvJTom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRADIENT BOOSTING**"
      ],
      "metadata": {
        "id": "AhLAc80HOlgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the packages\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making prediction on train and test data\n",
        "y_pred_train_g = gb_model.predict(X_train)\n",
        "y_pred_test_g = gb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "gxuwOHzsODRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",gb_model.score(X_train,y_train))\n",
        "#calculate MSE\n",
        "MSE_gb= mean_squared_error(y_train, y_pred_train_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_gb=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_gb= mean_absolute_error(y_train, y_pred_train_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_gb= r2_score(y_train, y_pred_train_g)\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score(y_train, y_pred_train_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "Pi28OlKCPQu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on the r2 score value of 0.87, it appears that our model has successfully captured a significant portion of the data variance. We can now  store this value in a dataframe for future comparisonns.**"
      ],
      "metadata": {
        "id": "HF9t53ZgPqH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the test set metrics value is a dataframe for later comparison\n",
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Gradient boosting regression ',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((r2_gb),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "-L1lBQqET4OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_gb= mean_squared_error(y_test, y_pred_test_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_gb=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_gb= mean_absolute_error(y_test, y_pred_test_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_gb= r2_score((y_test), (y_pred_test_g))\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "Wz-ZtxMjUN9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The test set's r2_score of 0.86 indicates that our linear model is effectively performing on the data. However, we need to examine our residuals graphically to determine whether there is any heteroscadasticity (unequal variance or scatter) present.**"
      ],
      "metadata": {
        "id": "0tFMoEplWPmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Gradient boosting regression ',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((r2_gb),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "51013ZYsQ3vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_g),(y_test)-(y_pred_test_g))"
      ],
      "metadata": {
        "id": "gopZHyMgRAmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.feature_importances_"
      ],
      "metadata": {
        "id": "CGVY6QeGRK3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "cyNAUrjqRTie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "TiAzdKNqRYiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.head()"
      ],
      "metadata": {
        "id": "R0hLlC7TRcjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)\n"
      ],
      "metadata": {
        "id": "iWlV0lI1Rvfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "orWIUsRFSBza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "1_oHK4DgSfzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jsD2mkeJTfZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. HYPERPARAMETER TUNING***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Hyperparameter tuning is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. The key to machine learning algorithm is hyperparameter tuning."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Using GridSearch CV**\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV helps to loop through predefined hyperparameters and fit the model on the training set. So, in the end, we can select the best parameters from the listed hyperparameters."
      ],
      "metadata": {
        "id": "K6quBmkhUqqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gradient Boosting Regressor with GridSearch CV**"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Provide a range of values for chosen hyperparameters**"
      ],
      "metadata": {
        "id": "EIYUGt82Pkkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ],
      "metadata": {
        "id": "Mor2o4q2WIaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_dict"
      ],
      "metadata": {
        "id": "JQWeKLN0WNT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Importing Gradient Boosting Regressor**"
      ],
      "metadata": {
        "id": "anU3GUg5WS1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "# Grid search\n",
        "gb_grid = GridSearchCV(estimator=gb_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2)\n",
        "\n",
        "gb_grid.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "id": "NX3k0H5fWvj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "JHmHuuUycAO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model = gb_grid.best_estimator_\n",
        "gb_grid.best_params_"
      ],
      "metadata": {
        "id": "fitS-nf1cN2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_g_g = gb_optimal_model.predict(X_train)\n",
        "y_pred_g_g= gb_optimal_model.predict(X_test)"
      ],
      "metadata": {
        "id": "5yadQjrydnyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",gb_optimal_model.score(X_train,y_train))\n",
        "MSE_gbh= mean_squared_error(y_train, y_pred_train_g_g)\n",
        "print(\"MSE :\",MSE_gbh)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gbh)\n",
        "print(\"RMSE :\",RMSE_gbh)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_train, y_pred_train_g_g)\n",
        "print(\"MAE :\",MAE_gbh)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score(y_train, y_pred_train_g_g)\n",
        "print(\"R2 :\",r2_gbh)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score(y_train, y_pred_train_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "vg_g = gb_optimal_model.predict(X_train)\n",
        "y_pred_g_g= gb_optimal_model.predict(X_test)"
      ],
      "metadata": {
        "id": "GQHr6ALpcgkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Gradient Boosting gridsearchcv ',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((r2_gbh),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
        "      }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "0Z3xNtR6d9XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "MSE_gbh= mean_squared_error(y_test, y_pred_g_g)\n",
        "print(\"MSE :\",MSE_gbh)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gbh)\n",
        "print(\"RMSE :\",RMSE_gbh)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_test, y_pred_g_g)\n",
        "print(\"MAE :\",MAE_gbh)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score((y_test), (y_pred_g_g))\n",
        "print(\"R2 :\",r2_gbh)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score(y_test, y_pred_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_g_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "vEj_HvgMd_FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Gradient Boosting gridsearchcv ',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((r2_gbh),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
        "      }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "ZSlhxx6wegfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_g_g),(y_test)-(y_pred_g_g), color='orange')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TgldxncOelYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model.feature_importances_"
      ],
      "metadata": {
        "id": "YDx2k6GAfNGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imp = gb_optimal_model.feature_importances_\n",
        "\n",
        "imp_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : imp}\n",
        "\n",
        "imp_df = pd.DataFrame(imp_dict)"
      ],
      "metadata": {
        "id": "zGsH2ynZfZqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imp_df['Feature Importance'] = round(imp_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "vqKt0baqfsV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imp_df.head()"
      ],
      "metadata": {
        "id": "lQoGc5tYgdsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imp_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "wJUFMwGTgmB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "hMYfve5cjSpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "imp = gb_model.feature_importances_\n",
        "indices = np.argsort(imp)"
      ],
      "metadata": {
        "id": "gbDrFcRZjm_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature importance')\n",
        "plt.barh(range(len(indices)),imp[indices], color='black', align = 'center')\n",
        "plt.yticks(range(len(indices)),[features[i] for i in indices])\n",
        "plt.xlabel('Relative importance')"
      ],
      "metadata": {
        "id": "iHRSBOiEkKke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During our analysis, we conducted an initial exploratory data analysis(EDA) on all the features in our dataset. Firstly, we analyzed our dependent variable 'Rented Bike Count' and applied transformation as necessary. We then examined the categorical  variables and removed those with a majority of one class. We also studied the numerical variables, calculated their correlations, distributions, and their relationships with the dependent variable. Additionally, we removed some numerical features that contained mostly 0 values and applied one-hot encoding to the categorical variables.\n",
        "\n",
        "Subsequently, we employed five  machine learning algorithm including Linear Regression, Lasso, Ridge, Elastic Net, and Gradient Booster. We\n",
        "also performed hyperparameter tuning to enhance the performance of our models. The evaluation of our models resulted in the following findings:"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the results of evaluation metric values for all models\n",
        "result = pd.concat([training_df,test_df],keys=['Training set','Test set'])\n",
        "result"
      ],
      "metadata": {
        "id": "m5YyYZlpoHQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Among all the models on the training set, the Gradient Boosting GridSearchCV has the lowest MAE, MSE, and RMSE, and the highest R2_score and Adjusted R2.\n",
        "\n",
        "2. The Linear Regression and Ridge Regression models have the same MAE,MSE,RMSE,R2_score, and Adjusted R2 on the training set and test set.\n",
        "\n",
        "3. Among all the models on the test set, the Gradient Boosting GridSearchCV has the lowest MAE,MSE and RMSE, and the highest R2_score and Adjusted R2.\n",
        "\n",
        "4. The Lasso regression has the highest MAE,MSE, and RMSE on both the training set and test set.\n",
        "\n",
        "5. The Elastic net regression has a similar performance on both the training set and test set.\n",
        "\n",
        "6. The Gradient Boosting regression has a lower performance than Gradient Boosting GridSearchCV on both the training set and test set.\n",
        "\n",
        "7. The Gradient Boosting GridSearchCV model performed the best on both the training and test sets based on its low MAE,MSE,RMSE and high R2_score and Adjusted R2.\n",
        "\n",
        "\n",
        "8. The Linear Regression model also performed well on both sets, although not as well as the Gradient Boosting GridSearchCV model.\n",
        "\n",
        "9. Lasso Regression had a higher error and a lower R2_score compared to the other models, indicating that it may not be the best model for this dataset.\n",
        "\n",
        "10. Elastic  net regression performed well but not as well as the Gradient Boosting GridSearchCV and Linear Regression models.\n",
        "\n",
        "11. No overfitting is observed in the dataset.\n",
        "\n",
        "12. Ridge regression had similar performance to Linear Regeression on both the training and test sets.\n",
        "\n",
        "13. The Gradient Boosting regression model had good performance on the training set, but slightly worse performance on the test set indicating some overfitting may have occured.\n",
        "\n",
        "Overall, the Gradient Boosting GridSearchCV model is the most promising model for this dataset based on the presented metrics.\n",
        "\n",
        "We can deploy this model.\n",
        "\n",
        "Although the current  analysis may be insightful, it is important to note that the dataset is time-dependent and variables such as temperature, windspeed, and solar radiation may not always remain consistent. As a result, there may be situations where the model falls to perform well. As the field of machine learning is constantly evolving, it is necessary to stay up-to-date with the latest developments and be prepared to handle unexpected scenarios. Maintaining a strong understanding of machine learning concepts will undoubtedly provide an advantage in staying ahead in the future."
      ],
      "metadata": {
        "id": "V2AWodByopao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}